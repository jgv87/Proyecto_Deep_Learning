{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSemm7zXRdpK"
      },
      "outputs": [],
      "source": [
        "# ÍNDICE :\n",
        "\n",
        "# 1.EXPLORACIÓN DE IMÁGENES\n",
        "# 2.PREPROCESADO DE IMÁGENES\n",
        "# 3.MODELADO DE IMÁGENES\n",
        "\n",
        "# 4.EXPLORACIÓN DE DATOS TABULARES\n",
        "# 5.PREPROCESADO DE DATOS TABULARES\n",
        "# 6.MODELADO DE DATOS TABULARES\n",
        "\n",
        "# 7.EARLY FUSIÓN & LATE FUSIÓN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAgPb0a9voAz"
      },
      "source": [
        "# **1 - EXPLORACIÓN DE IMÁGENES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oABz1mWkvlC2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Lectura CSV desde Google Drive\n",
        "ruta_csv = '/content/drive/MyDrive/Colab Notebooks/airbnb-listings.csv'\n",
        "airbnb_data = pd.read_csv(ruta_csv,  sep=';')\n",
        "\n",
        "# Total de filas del dataframe\n",
        "total_filas = len(airbnb_data)\n",
        "print(\"El total de filas en el DataFrame es:\", total_filas, \"\\n\")\n",
        "\n",
        "\"\"\" Queremos obtener información sobre los datos de imágenes y precios.\n",
        "    En este caso, la exploración de NA lo realizo sobre todos los datos,\n",
        "    puesto que es posible que existan NA en train y no en test o validacion \"\"\"\n",
        "\n",
        "# Verificar si existen valores nulos en la columna 'price'\n",
        "if airbnb_data['Price'].isnull().any():\n",
        "    print(\"La columna 'Price' contiene valores nulos.\")\n",
        "else:\n",
        "    print(\"La columna 'Price' no contiene valores nulos.\")\n",
        "\n",
        "# Verificar si existen valores nulos en la columna 'price'\n",
        "if airbnb_data['Thumbnail Url'].isnull().any():\n",
        "    print(\"La columna 'Thumbnail Url' contiene valores nulos.\")\n",
        "else:\n",
        "    print(\"La columna 'Thumbnail Url' no contiene valores nulos.\")\n",
        "\n",
        "print()\n",
        "\n",
        "\"\"\" Cogemos solo con los datos de Madrid puesto que el resto de ciudades tiene muy pocas imágenes\n",
        "    En el módulo de ML no era determinante la ubicación para el precio, pero creo que de esta forma\n",
        "    elimino el poco ruido que pueda generar \"\"\"\n",
        "\n",
        "column_values = airbnb_data['City'].value_counts()\n",
        "print(column_values.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MELE0WbqQBtx"
      },
      "source": [
        "# **2 - PREPROCESADO DE IMÁGENES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKpviVHgNyfZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Eliminamos NA de las columnas 'Thumbnail Url' y 'Price'\n",
        "airbnb_data = airbnb_data.dropna(subset=['Thumbnail Url'])\n",
        "airbnb_data = airbnb_data.dropna(subset=['Price'])\n",
        "total_filas = len(airbnb_data)\n",
        "print(\"El total de filas quitando los NA en 'Thumbnail Url' y 'Price' en el DataFrame es:\", total_filas, \"\\n\")\n",
        "\n",
        "# Solo los datos de Madrid\n",
        "airbnb_data = airbnb_data[airbnb_data['City'] == 'Madrid']\n",
        "print(f'Dimensiones de solo Madrid: {airbnb_data.shape}', \"\\n\")\n",
        "\n",
        "\"\"\" Lo queremos es que el CSV contenga los mismos índices\n",
        "    que el de las imágenes que se hayan podido descargar \"\"\"\n",
        "\n",
        "# Eliminamos todas las filas del CSV donde no se haya podido descargar la imagen\n",
        "image_destination_dir = '/content/drive/MyDrive/imagenes_madrid'\n",
        "\n",
        "# Lista para almacenar los índices de las filas válidas\n",
        "valid_rows = []\n",
        "\n",
        "# Itera sobre las filas del DataFrame\n",
        "for index, row in airbnb_data.iterrows():\n",
        "    # Obtiene el nombre de archivo basado en el índice\n",
        "    image_filename = f'image_{index}.jpg'\n",
        "\n",
        "    # Verifica si la imagen se descargó correctamente\n",
        "    if os.path.isfile(os.path.join(image_destination_dir, image_filename)):\n",
        "        valid_rows.append(index)\n",
        "\n",
        "# Sobreescribimos el DataFrame solo con las filas válidas en imágenes y precios\n",
        "airbnb_data = airbnb_data.loc[valid_rows]\n",
        "\n",
        "ruta_imagenes = '/content/drive/MyDrive/imagenes_madrid/'\n",
        "ruta_precios = '/content/drive/MyDrive/precios_madrid/'\n",
        "\n",
        "# Obtener la lista de archivos en la carpeta\n",
        "lista_imagenes = os.listdir(ruta_imagenes)\n",
        "lista_precios = os.listdir(ruta_precios)\n",
        "\n",
        "# Obtener el número de archivos en la carpeta\n",
        "num_imagenes = len(lista_imagenes)\n",
        "num_precios = len(lista_precios)\n",
        "\n",
        "# Comprobamos que exista el mismo número de filas en CSV e imágenes\n",
        "print(f\"La carpeta de imagenes contiene {num_imagenes} imagenes.\")\n",
        "print(f\"La carpeta de precios contiene {num_precios} precios.\")\n",
        "print(\"El dataframe contiene:\", airbnb_data.shape[0], \"filas\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pasamos a un array el número de precios e imágenes que deseamos\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\"\"\" Descargamos las imágenes y precios de la carpeta de Google Drive y las pasamos a un array\n",
        "    Posteriormente a arreglos de numpy para poder trabajar con ellos\n",
        "    Añado una opción de descarga en la que puedes elegir el número que quieres utlizar\n",
        "    de imágenes, ya que descargar todas lleva su tiempo \"\"\"\n",
        "\n",
        "image_folder = '/content/drive/MyDrive/imagenes_madrid'\n",
        "price_folder = '/content/drive/MyDrive/precios_madrid'\n",
        "\n",
        "image_files = os.listdir(image_folder)\n",
        "price_files = os.listdir(price_folder)\n",
        "\n",
        "images = []\n",
        "prices = []\n",
        "\n",
        "for i, (image_file, price_file) in enumerate(zip(image_files, price_files)):\n",
        "    if i >= 2000: # Elegimos cantidad de imágenes\n",
        "        break\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    price_path = os.path.join(price_folder, price_file)\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    images.append(image)\n",
        "\n",
        "    price = np.loadtxt(price_path)\n",
        "    prices.append(price)\n",
        "\n",
        "# Convertir las listas a arreglos numpy\n",
        "images = np.array(images)\n",
        "prices = np.array(prices)\n",
        "\n",
        "\"\"\" # Pasamos a un array precios e imágenes --> todas las imágenes\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image_folder = '/content/drive/MyDrive/imagenes_madrid'\n",
        "price_folder = '/content/drive/MyDrive/precios_madrid'\n",
        "\n",
        "image_files = os.listdir(image_folder)\n",
        "price_files = os.listdir(price_folder)\n",
        "\n",
        "images = []\n",
        "prices = []\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    image = cv2.imread(image_path)\n",
        "    images.append(image)\n",
        "\n",
        "for price_file in price_files:\n",
        "    price_path = os.path.join(price_folder, price_file)\n",
        "    price = np.loadtxt(price_path)\n",
        "    prices.append(price)\n",
        "\n",
        "# Convertir las listas a arreglos numpy\n",
        "images = np.array(images)\n",
        "prices = np.array(prices) \"\"\""
      ],
      "metadata": {
        "id": "izM8c_rGKYVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He5JgICONynu"
      },
      "outputs": [],
      "source": [
        "# Divisón, redimensión y normalización de datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Semilla\n",
        "SEED = 5\n",
        "\n",
        "# Dividimos los datos en conjuntos de entrenamiento, validación y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, prices, test_size=0.2, random_state=SEED)\n",
        "validation_split = 0.15\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_split, random_state=SEED)\n",
        "\n",
        "\"\"\" Vamos usar redes convolucionales para las imágenes, probaremos una red desde cero y posteriormente\n",
        "    un modelo preentrenado,tanto en forma Tranfer-learnig como Fine-tuning \"\"\"\n",
        "\n",
        "\"\"\" Dadas las limitaciones computacionales de mi ordenador y el gran número de imágenes a procesar\n",
        "    quería la arquitectura SqueezeNet ya que es una red muy ligera y con buenos resultados,\n",
        "    pero he tenido problemas con las versiones por lo que probaremos con el modelo ResNet que\n",
        "    ocupa menos espacio y permite dimensiones más pequeñas que otros modelos \"\"\"\n",
        "\n",
        "# Pasamos a float\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Redimensión y normalización\n",
        "input_shape = (180, 180, 3)\n",
        "\n",
        "X_train_resized = []\n",
        "for img in X_train:\n",
        "  X_train_resized.append(np.resize(img, input_shape) / 255.)\n",
        "X_train_resized = np.array(X_train_resized)\n",
        "print(\"Forma de X_train en imágenes:\", X_train_resized.shape)\n",
        "\n",
        "X_val_resized = []\n",
        "for img in X_val:\n",
        "  X_val_resized.append(np.resize(img, input_shape) / 255.)\n",
        "X_val_resized = np.array(X_val_resized)\n",
        "print(\"Forma de X_val en imágenes:\", X_val_resized.shape)\n",
        "\n",
        "X_test_resized = []\n",
        "for img in X_test:\n",
        "  X_test_resized.append(np.resize(img, input_shape) / 255.)\n",
        "X_test_resized = np.array(X_test_resized)\n",
        "print(\"Forma de X_test en imágenes:\", X_test_resized.shape)\n",
        "\n",
        "# Conversión de etiquetas\n",
        "scaler = MinMaxScaler()\n",
        "y_train_resized = scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val_resized = scaler.transform(y_val.reshape(-1, 1))\n",
        "y_test_resized = scaler.transform(y_test.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZtacVGXaxdD"
      },
      "source": [
        "# **3 - MODELADO DE IMÁGENES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p-PSuKdNyqM"
      },
      "outputs": [],
      "source": [
        "# Búsqueda mejores Hiperparámetros\n",
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from tensorflow.keras import callbacks, optimizers, Model\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\" Además de estos hiperparámetros podemos buscar otros\n",
        "    como units o layers , pero tardaría bastante más \"\"\"\n",
        "\n",
        "# Definir la función de entrenamiento y evaluación con los hiperparámetros como argumentos\n",
        "def train_and_evaluate(params):\n",
        "\n",
        "    # Construir el modelo con los hiperparámetros\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    last = base_model.layers[-1].output\n",
        "    x = Flatten()(last)\n",
        "    x = Dense(1000, activation='relu')(x)\n",
        "  # x = Dropout(params['dropout_rate'])(x)\n",
        "    x = Dense(1, activation='linear', name='regression')(x)\n",
        "    model = Model(base_model.input, x)\n",
        "\n",
        "    # Compilamos el modelo con Adam como optimizador\n",
        "    model.compile(optimizer=Adam(params['learning_rate']), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Definir Early Stopping\n",
        "    early_stopping =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='auto')\n",
        "\n",
        "    # Entrenanamos el modelo con los hiperparámetros\n",
        "    history = model.fit(X_train_resized, y_train_resized,\n",
        "                        validation_data=(X_val_resized, y_val_resized),\n",
        "                        epochs=params['epochs'],\n",
        "                        batch_size=params['batch_size'],\n",
        "                        verbose=0,\n",
        "                        callbacks=[early_stopping])\n",
        "\n",
        "    # Obtener el valor de val_loss en la última época\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    # Devolver el val_loss a minimizar\n",
        "    return val_loss\n",
        "\n",
        "# Definir el espacio de búsqueda de hiperparámetros\n",
        "space = {\n",
        "  # 'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5), -- > no tenemos overfitting\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.01),\n",
        "    'epochs': hp.choice('epochs', [2, 4]),\n",
        "    'batch_size': hp.choice('batch_size', [5, 10])\n",
        "}\n",
        "\n",
        "\"\"\" El space lo ajustamos en base a las pruebas que hemos\n",
        "    ido realizando \"\"\"\n",
        "\n",
        "# Función objetivo para minimizar el val_loss directamente\n",
        "def objective(params):\n",
        "    val_loss = train_and_evaluate(params)\n",
        "    return val_loss\n",
        "\n",
        "# Configurar la búsqueda con Hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "# Imprimir los mejores hiperparámetros encontrados\n",
        "print('Mejores hiperparámetros:')\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ub5v4ioC_u9"
      },
      "outputs": [],
      "source": [
        "# Red Convolucional\n",
        "from tensorflow.keras import optimizers, Model, layers\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\" He decidido usar una regresión lineal para predecir el precio de las casas porque creo que\n",
        "    el objetivo es predecir el precio de las habitaciones. Si lo clasifico en categorías\n",
        "    estaríamos buscando tendencias de precios, pero no su valor real.\n",
        "    Por otro lado, considero que teniendo 8433 filas no es necesario realizar 'Data augmentation' \"\"\"\n",
        "\n",
        "\"\"\" Con una red convolucional creada desde cero se consiguen peores resutlados\n",
        "    que con modelos preentrenados, pero sobretodo consume bastante más RAM \"\"\"\n",
        "\n",
        "epochs = 3\n",
        "batch_size = 2\n",
        "lr = 0.001\n",
        "\n",
        "# Inicializamos el modelo\n",
        "model = Sequential()\n",
        "\n",
        "# Definimos una capa convolucional\n",
        "model.add(Conv2D(128, kernel_size=(5, 5), activation='relu', input_shape=(input_shape)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Definimos una segunda capa convolucional\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Añadimos nuestro clasificador\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='linear', name='regression'))\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer=Adam(lr), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train_resized, y_train_resized, validation_data=(X_val_resized, y_val_resized), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluamos el MSE y el MAE en test\n",
        "scores = model.evaluate(X_test_resized, y_test_resized, verbose=1)\n",
        "print('Test MSE:', scores[0])\n",
        "print('Test MAE:', scores[1])\n",
        "\n",
        "# Predicciones en train\n",
        "y_train_pred = model.predict(X_train_resized)\n",
        "r2_train = r2_score(y_train_resized, y_train_pred)\n",
        "print(f'R^2 Train: {r2_train}')\n",
        "\n",
        "# Predicciones en test\n",
        "y_test_pred = model.predict(X_test_resized)\n",
        "r2_test = r2_score(y_test_resized, y_test_pred)\n",
        "print(f'R^2 Test: {r2_test}', \"\\n\")\n",
        "\n",
        "# Deshacer el escalado de etiquetas\n",
        "\"\"\" y_train = scaler.inverse_transform(y_train_resized)\n",
        "    y_val = scaler.inverse_transform(y_val_resized)\n",
        "    y_test = scaler.inverse_transform(y_test_resized) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLZseY0pNysj"
      },
      "outputs": [],
      "source": [
        "# Transfer learning\n",
        "from tensorflow.keras import optimizers, Model\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "\"\"\" Dentro de Resnet exiten varias versiones en función de las capas (18-152),\n",
        "    cogeremos algo intermedio en este caso Resnet50 \"\"\"\n",
        "\n",
        "# Hiperparámetros\n",
        "epochs = 3\n",
        "batch_size = 2\n",
        "lr = 0.001\n",
        "\n",
        "# Construimos el modelo base\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Congelamos todas las capas de nuestro base_model para que no se entrenen\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Cogemos la última capa del model y le añadimos nuestra capa de regresión lineal\n",
        "last = base_model.layers[-1].output\n",
        "x = Flatten()(last)\n",
        "x = Dense(1000, activation='relu')(x)\n",
        "# x = Dropout(0.2161074859485292)(x) ---> no tenemos overfitting\n",
        "x = Dense(1, activation='linear', name='regression')(x)  # Capa de regresión lineal con 1 unidad\n",
        "model = Model(base_model.input, x)\n",
        "\n",
        "\"\"\" No estoy teniendo overfitting por lo que no implementaremos nigún método\n",
        "    de regularización (Lasso/Ridge) ni Dropout \"\"\"\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer=Adam(lr), loss='mean_squared_error', metrics=['mae'])  # Utilizamos MSE y MAE para regresión\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train_resized, y_train_resized, validation_data=(X_val_resized, y_val_resized), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluamos el MSE y el MAE en test\n",
        "scores = model.evaluate(X_test_resized, y_test_resized, verbose=1)\n",
        "print('Test MSE:', scores[0])\n",
        "print('Test MAE:', scores[1])\n",
        "\n",
        "# Predicciones en train\n",
        "y_train_pred = model.predict(X_train_resized)\n",
        "r2_train = r2_score(y_train_resized, y_train_pred)\n",
        "print(f'R^2 Train: {r2_train}')\n",
        "\n",
        "# Predicciones en test\n",
        "y_test_pred = model.predict(X_test_resized)\n",
        "r2_test = r2_score(y_test_resized, y_test_pred)\n",
        "print(f'R^2 Test: {r2_test}', \"\\n\")\n",
        "\n",
        "# Deshacer el escalado de etiquetas\n",
        "\"\"\" y_train = scaler.inverse_transform(y_train_resized)\n",
        "    y_val = scaler.inverse_transform(y_val_resized)\n",
        "    y_test = scaler.inverse_transform(y_test_resized) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA7VafQaNyu6"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning\n",
        "from tensorflow.keras import optimizers, Model\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\"\"\" Mismos pasos que en Transfer-learning, pero en este caso le dejamos\n",
        "    la última capa del modelo para que entrene \"\"\"\n",
        "\n",
        "\"\"\" Resultados bastante similares que con Transfer-learning \"\"\"\n",
        "\n",
        "\"\"\" Como conclusión diría que en redes convolucionales he conseguido mejores\n",
        "    resultados con menos imágenes y más shape que con todas las imágenes y\n",
        "    menos resolución \"\"\"\n",
        "\n",
        "# Hiper-parámetros\n",
        "epochs = 3\n",
        "batch_size = 2\n",
        "lr = 0.001\n",
        "\n",
        "# Construimos el modelo base\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# permitimos que, además de la etapa de clasificación, se entrenen también el último bloque convolucional\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "  if layer.name == \"conv5_block3_3_conv\":\n",
        "    layer.trainable = True\n",
        "\n",
        "# cogemos la última capa del model y le añadimos nuestra capa de regresión lineal\n",
        "last = base_model.layers[-1].output\n",
        "x = Flatten()(last)\n",
        "x = Dense(500, activation='relu')(x)\n",
        "x = Dense(1, activation='linear', name='regression')(x)  # Capa de regresión lineal con 1 unidad\n",
        "# x = Dropout(0.2161074859485292)(x) ---> no tenemos overfitting\n",
        "model = Model(base_model.input, x)\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer=Adam(lr), loss='mean_squared_error', metrics=['mae'])  # Utilizamos MSE y MAE para regresión\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train_resized, y_train_resized, validation_data=(X_val_resized, y_val_resized),  epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluamos el MSE y el MAE en test\n",
        "scores = model.evaluate(X_test_resized, y_test_resized, verbose=1)\n",
        "print('Test MSE:', scores[0])\n",
        "print('Test MAE:', scores[1])\n",
        "\n",
        "# Predicciones en train\n",
        "y_train_pred = model.predict(X_train_resized)\n",
        "r2_train = r2_score(y_train_resized, y_train_pred)\n",
        "print(f'R^2 Train: {r2_train}')\n",
        "\n",
        "# Predicciones en test\n",
        "y_test_pred = model.predict(X_test_resized)\n",
        "r2_test = r2_score(y_test_resized, y_test_pred)\n",
        "print(f'R^2 Test: {r2_test}')\n",
        "\n",
        "# Deshacer el escalado de etiquetas\n",
        "\"\"\" y_train = scaler.inverse_transform(y_train_resized)\n",
        "    y_val = scaler.inverse_transform(y_val_resized)\n",
        "    y_test = scaler.inverse_transform(y_test_resized) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlhD0Hp7-vhl"
      },
      "source": [
        "# **4 - EXPLORACIÓN DE DATOS TABULARES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UuQwChHzmwE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Selección de columnas propuestas en la práctica\n",
        "selected_columns = ['Price', 'Property Type', 'Room Type', 'Cancellation Policy', 'Accommodates', 'Bathrooms',\n",
        "                    'Bedrooms', 'Beds', 'Guests Included', 'Extra People', 'Minimum Nights',\n",
        "                    'Maximum Nights', 'Number of Reviews', 'Host Total Listings Count']\n",
        "\n",
        "# Cogemos el mismo DataFrame ya procesado en imágenes con las columnas seleccionadas\n",
        "airbnb_data = airbnb_data[selected_columns]\n",
        "\n",
        "\"\"\" Primero arrancar la celda de preprocesado de imágenes para tener las mismas filas\n",
        "    Podemos elegir menos filas, pero tienen que ser las mismas que imágenes,\n",
        "    de lo contrario early fusión y late fusión no funcionarán \"\"\"\n",
        "\n",
        "# Elegimos el numero de filas que deseamos\n",
        "# Tiene que coincidir con el número de imágenes\n",
        "airbnb_data = airbnb_data.head(2000)\n",
        "\n",
        "# Semilla\n",
        "SEED = 40\n",
        "\n",
        "# Dividimos los datos en conjuntos de entrenamiento, validación y test\n",
        "train_tab, test_tab = train_test_split(airbnb_data, test_size=0.2, random_state=SEED)\n",
        "train_tab, val_tab = train_test_split(train_tab, test_size=0.15, random_state=SEED)\n",
        "\n",
        "# Verificamos formas de los conjuntos de entrenamiento, validación y test\n",
        "print(\"Forma de train en datos tabulares:\", train_tab.shape)\n",
        "print(\"Forma de val en datos tabulares:\", val_tab.shape)\n",
        "print(\"Forma de test en datos tabulares:\", test_tab.shape, \"\\n\")\n",
        "\n",
        "airbnb_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L1kmDlE5NaJ"
      },
      "outputs": [],
      "source": [
        "# Mapa de correlación entre variables de train\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = np.abs(train_tab.drop(['Price', 'Room Type', 'Property Type', 'Cancellation Policy'], axis=1).corr())\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.zeros_like(corr, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n",
        "            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\"\"\" Variables algo correlacionadas'. Si tuvieramos overfitting\n",
        "    utilizaríamos métodos de regularización \"\"\"\n",
        "\n",
        "# Información de train\n",
        "train_tab.info()\n",
        "\n",
        "print()\n",
        "\n",
        "\"\"\" Vemos que no todos los datos son del mismo tipo\n",
        "    Les aplicaremos a todos tipo 'float' \"\"\"\n",
        "\n",
        "# Porcentaje de nulos de train\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "valores_nulos = train_tab.isnull().sum()\n",
        "porcentaje_nulos = (valores_nulos / total_filas) * 100\n",
        "print(\"Porcentaje de nulos:\", porcentaje_nulos, \"\\n\")\n",
        "\n",
        "\"\"\" Al existir pocos porcentajes de nulos los sustituiremos por la moda \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5skHiy582tU"
      },
      "source": [
        "# **5 - PREPROCESADO DATOS TABULARES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxRHyyrSzm0w"
      },
      "outputs": [],
      "source": [
        "# Tomamos decisiones en base a lo que hemos visto en train\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Preprocesamiento de test\n",
        "\n",
        "# Categorizamos variables\n",
        "label_encoder = LabelEncoder()\n",
        "train_tab['Room Type'] = label_encoder.fit_transform(train_tab['Room Type'])\n",
        "train_tab['Property Type'] = label_encoder.fit_transform(train_tab['Property Type'])\n",
        "train_tab['Cancellation Policy'] = label_encoder.fit_transform(train_tab['Cancellation Policy'])\n",
        "\n",
        "# Convertir las columnas de tipo object a int64\n",
        "float_columns = train_tab.select_dtypes(include=['object']).columns\n",
        "train_tab[float_columns] = train_tab[float_columns].astype('float64')\n",
        "\n",
        "# Rellenamos valores faltantes con la moda\n",
        "column_modes = train_tab.mode().iloc[0]\n",
        "train_tab = train_tab.fillna(column_modes)\n",
        "\n",
        "# Obtener las variables de entrada y salida\n",
        "X_train_tab = train_tab.iloc[:, 1:].values  # Variables de entrada\n",
        "y_train_tab = train_tab.iloc[:, 0].values  # Variable de salida 'Price'\n",
        "\n",
        "# Normalización de variables de entrada y etiquetas\n",
        "scaler_entradas = MinMaxScaler()\n",
        "X_train_tab = scaler_entradas.fit_transform(X_train_tab)\n",
        "\n",
        "scaler_etiquetas = MinMaxScaler()\n",
        "y_train_tab = scaler_etiquetas.fit_transform(y_train_tab.reshape(-1, 1))\n",
        "\n",
        "# Preprocesamiento de val\n",
        "\n",
        "# Categorizamos variables\n",
        "val_tab['Room Type'] = label_encoder.fit_transform(val_tab['Room Type'])\n",
        "val_tab['Property Type'] = label_encoder.fit_transform(val_tab['Property Type'])\n",
        "val_tab['Cancellation Policy'] = label_encoder.fit_transform(val_tab['Cancellation Policy'])\n",
        "\n",
        "# Convertir las columnas de tipo object a int64\n",
        "float_columns = val_tab.select_dtypes(include=['object']).columns\n",
        "val_tab[float_columns] = val_tab[float_columns].astype('float64')\n",
        "\n",
        "# Rellenamos valores faltantes con la moda\n",
        "column_modes = val_tab.mode().iloc[0]\n",
        "val_tab = val_tab.fillna(column_modes)\n",
        "\n",
        "# Obtener las variables de entrada y salida\n",
        "X_val_tab = val_tab.iloc[:, 1:].values  # Variables de entrada\n",
        "y_val_tab = val_tab.iloc[:, 0].values # Variable de salida 'Price'\n",
        "\n",
        "# Normalización de variables de entrada y etiquetas\n",
        "X_val_tab = scaler_entradas.transform(X_val_tab)\n",
        "y_val_tab = scaler_etiquetas.transform(y_val_tab.reshape(-1, 1))\n",
        "\n",
        "# Preprocesamiento de test\n",
        "\n",
        "# Categorizamos variables\n",
        "test_tab['Room Type'] = label_encoder.fit_transform(test_tab['Room Type'])\n",
        "test_tab['Property Type'] = label_encoder.fit_transform(test_tab['Property Type'])\n",
        "test_tab['Cancellation Policy'] = label_encoder.fit_transform(test_tab['Cancellation Policy'])\n",
        "\n",
        "# Convertir las columnas de tipo object a int64\n",
        "float_columns = test_tab.select_dtypes(include=['object']).columns\n",
        "test_tab[float_columns] = test_tab[float_columns].astype('float64')\n",
        "\n",
        "# Rellenamos valores faltantes con la moda\n",
        "column_modes = test_tab.mode().iloc[0]\n",
        "test_tab = test_tab.fillna(column_modes)\n",
        "\n",
        "# Obtener las variables de entrada y salida\n",
        "X_test_tab = test_tab.iloc[:, 1:].values  # Variables de entrada\n",
        "y_test_tab = test_tab.iloc[:, 0].values  # Variable de salida 'Price'\n",
        "\n",
        "# Normalización de variables de entrada y etiquetas\n",
        "X_test_tab = scaler_entradas.transform(X_test_tab)\n",
        "y_test_tab = scaler_etiquetas.transform(y_test_tab.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li0jSWJhPg6P"
      },
      "source": [
        "# **6 - MODELADO DATOS TABULARES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ieuhibgzm3F"
      },
      "outputs": [],
      "source": [
        "# Búsqueda mejores Hiperparámetros\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "\n",
        "# Definir la función de entrenamiento y evaluación con los hiperparámetros como argumentos\n",
        "def train_and_evaluate(params):\n",
        "\n",
        "    # Creamos la red neuronal\n",
        "    model = Sequential()\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Agregamos las capas ocultas\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(1, input_shape=(13,), activation='linear', name='regression'))\n",
        "\n",
        "    # Compilamos el modelo con Adam como optimizador\n",
        "    model.compile(optimizer=Adam(learning_rate=params['learning_rate']), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Definimos Early Stopping\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='auto')\n",
        "\n",
        "    # Entrenamos el modelo\n",
        "    history = model.fit(X_train_tab, y_train_tab,\n",
        "                        validation_data=(X_val_tab, y_val_tab),\n",
        "                        epochs=params['epochs'],\n",
        "                        batch_size=params['batch_size'],\n",
        "                        verbose=0,\n",
        "                        callbacks=[early_stopping])\n",
        "\n",
        "    # Obtenemos el val_loss en la última época\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    # Devolvemos el val_loss a minimizar\n",
        "    return val_loss\n",
        "\n",
        "# Definir el espacio de búsqueda de hiperparámetros\n",
        "space = {\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.01),\n",
        "    'epochs': hp.choice('epochs', [3, 5]),\n",
        "    'batch_size': hp.choice('batch_size', [2, 5])\n",
        "}\n",
        "\n",
        "# Función objetivo para minimizar el val_loss\n",
        "def objective(params):\n",
        "    val_loss = train_and_evaluate(params)\n",
        "    return val_loss\n",
        "\n",
        "# Configurar la búsqueda con Hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "# Imprimir los mejores hiperparámetros encontrados\n",
        "print('Mejores hiperparámetros:')\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKMWlL6Rzm5e"
      },
      "outputs": [],
      "source": [
        "# Creamos la red neuronal\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\"\"\" Tal y como hemos procedido en las imágenes usaremos regresión lineal\n",
        "    para las predicciones \"\"\"\n",
        "\n",
        "# Hiper-parámetros\n",
        "n_epochs = 3\n",
        "batch_size = 2\n",
        "lr = 0.001\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1, input_shape=(13,), activation='linear', name='regression'))\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer=Adam(lr), loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train_tab, y_train_tab, validation_data=(X_val_tab, y_val_tab), epochs=n_epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluamos el MSE y el MAE en el conjunto de prueba\n",
        "scores = model.evaluate(X_test_tab, y_test_tab, verbose=1)\n",
        "print('Test MSE:', scores[0])\n",
        "print('Test MAE:', scores[1], \"\\n\")\n",
        "\n",
        "# Predicciones en train\n",
        "y_train_pred = model.predict(X_train_tab)\n",
        "r2_train = r2_score(y_train_tab, y_train_pred)\n",
        "print(f'R^2 Train: {r2_train}')\n",
        "\n",
        "# Predicciones en test\n",
        "y_test_pred = model.predict(X_test_tab)\n",
        "r2_test = r2_score(y_test_tab, y_test_pred)\n",
        "print(f'R^2 Test: {r2_test}')\n",
        "\n",
        "# Deshacer el escalado de etiquetas\n",
        "\"\"\" y_train_tab = scaler.inverse_transform(y_train_tab)\n",
        "    y_val_tab = scaler.inverse_transform(y_val_tab)\n",
        "    y_test_tab = scaler.inverse_transform(y_test_tab) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJWHmQ2D0gEq"
      },
      "source": [
        "# **7 - EARLY FUSION  &  LATE FUSIÓN**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywsgOOFtT5jM"
      },
      "outputs": [],
      "source": [
        "# Búsqueda mejores Hiperparámetros\n",
        "import numpy as np\n",
        "from tensorflow.keras import callbacks, optimizers, Model\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Concatenate, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "\n",
        "# Definir la función de entrenamiento y evaluación con los hiperparámetros como argumentos\n",
        "def train_and_evaluate(params):\n",
        "\n",
        "    # Construir el modelo base de imágenes\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Congelar todas las capas del modelo base para que no se entrenen\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    n_features = 13\n",
        "\n",
        "    # Capa de entrada para características tabulares\n",
        "    input_tabular = Input(shape=(n_features,))\n",
        "\n",
        "    # Capa de aplanado y capas densas para características de imágenes\n",
        "    image_features = base_model.output\n",
        "    image_features = Flatten()(image_features)\n",
        "    image_features = Dense(100)(image_features)\n",
        "\n",
        "    # Capa de fusión de características\n",
        "    merged_features = Concatenate()([input_tabular, image_features])\n",
        "\n",
        "    # Capa de regresión lineal\n",
        "    output_layer = Dense(1, activation='linear', name='regression')(merged_features)\n",
        "\n",
        "    # Modelo final de early fusion\n",
        "    model = Model(inputs=[base_model.input, input_tabular], outputs=output_layer)\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer=Adam(params['learning_rate']), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Definir Early Stopping\n",
        "    early_stopping =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, mode='auto')\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model.fit([X_train_resized, X_train_tab], y_train_tab,\n",
        "              validation_data=([X_val_resized, X_val_tab], y_val_tab),\n",
        "              epochs = params['epochs'], batch_size = params['batch_size'],\n",
        "              verbose=0, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de validación\n",
        "    val_loss = model.evaluate([X_val_resized, X_val_tab], y_val_tab, verbose=0)[0]\n",
        "\n",
        "    # Devolver el val_loss a minimizar\n",
        "    return val_loss\n",
        "\n",
        "# Definir el espacio de búsqueda de hiperparámetros\n",
        "space = {\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.01),\n",
        "    'epochs': hp.choice('epochs', [3, 5]),\n",
        "    'batch_size': hp.choice('batch_size', [2, 5])\n",
        "}\n",
        "\n",
        "# Función objetivo para minimizar el val_loss\n",
        "def objective(params):\n",
        "    val_loss = train_and_evaluate(params)\n",
        "    return val_loss\n",
        "\n",
        "# Configurar la búsqueda con Hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "# Imprimir los mejores hiperparámetros encontrados\n",
        "print('Mejores hiperparámetros:')\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhzaW3tr4xJ8"
      },
      "outputs": [],
      "source": [
        "# Early-fusión\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Concatenate, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\"\"\" En este caso, realizamos la extracción de características de imágenes y de\n",
        "    datos tabulares para unirlas y realizar una predicción con regresión lineal \"\"\"\n",
        "\n",
        "\"\"\" También utlizaremos Resnet50 como modelo preentrenado \"\"\"\n",
        "\n",
        "\"\"\" Resultados bastante peores que con Late-fusión \"\"\"\n",
        "\n",
        "# Hiper-parámetros\n",
        "epochs = 3\n",
        "batch_size = 2\n",
        "lr = 0.001\n",
        "\n",
        "# Construimos el modelo base de imágenes\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Congelamos todas las capas del modelo base para que no se entrenen\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "n_features = 13\n",
        "\n",
        "# Capa de entrada para características tabulares\n",
        "input_tabular = Input(shape=(n_features,))\n",
        "\n",
        "# Capa de aplanado y capas densas para características de imágenes\n",
        "image_features = base_model.output\n",
        "image_features = Flatten()(image_features)\n",
        "\n",
        "# Capa de fusión de características\n",
        "merged_features = Concatenate()([input_tabular, image_features])\n",
        "\n",
        "# Capa de regresión lineal\n",
        "output_layer = Dense(1, activation='linear', name='regression')(merged_features)\n",
        "\n",
        "# Modelo final de early fusion\n",
        "model = Model(inputs=[base_model.input, input_tabular], outputs=output_layer)\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer=Adam(lr), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit([X_train_resized, X_train_tab], y_train_tab, validation_data=([X_val_resized, X_val_tab], y_val_tab), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluamos el modelo en el conjunto de prueba\n",
        "scores = model.evaluate([X_test_resized, X_test_tab], y_test_tab, verbose=1)\n",
        "print('Test MSE:', scores[0])\n",
        "print('Test MAE:', scores[1])\n",
        "\n",
        "# Predicciones en train\n",
        "y_train_pred = model.predict([X_train_resized, X_train_tab])\n",
        "r2_train = r2_score(y_train_tab, y_train_pred)\n",
        "print(f'R^2 Train: {r2_train}')\n",
        "\n",
        "# Predicciones en test\n",
        "y_test_pred = model.predict([X_test_resized, X_test_tab])\n",
        "r2_test = r2_score(y_test_tab, y_test_pred)\n",
        "print(f'R^2 Test: {r2_test}')\n",
        "\n",
        "# Deshacer el escalado de etiquetas\n",
        "\"\"\" y_train_tab = scaler.inverse_transform(y_train_tab)\n",
        "    y_val_tab = scaler.inverse_transform(y_val_tab)\n",
        "    y_test_tab = scaler.inverse_transform(y_test_tab) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U76nRsXLmXVV"
      },
      "outputs": [],
      "source": [
        "# Late-fusion\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\"\"\" Parecido al caso de Eaely-fusión, pero ahora no unimos las características,\n",
        "    sino que unimos las predicciones que han realizado ambos por separado en una\n",
        "    nueva predicción \"\"\"\n",
        "\n",
        "\"\"\" Es curioso que al probar como etiqueta a predecir las imágenes en lugar de datos\n",
        "    tabulares tampoco han sido unos resultados muy por debajo, aun teniendo mucha\n",
        "    menos información para aprender. No obstante, he preferido dejar los datos\n",
        "    tabulares como etiqueta de predicción \"\"\"\n",
        "\n",
        "# Hiper-parámetros\n",
        "epochs = 3\n",
        "batch_size = 2\n",
        "lr = 0.001\n",
        "\n",
        "# Construir el modelo base de imágenes\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Congelar todas las capas del modelo base para que no se entrenen\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "n_features = 13\n",
        "\n",
        "# Capa de entrada para características tabulares\n",
        "input_tabular = Input(shape=(n_features,))\n",
        "\n",
        "# Capas densas para características tabulares\n",
        "tabular_features = Dense(64, activation='relu')(input_tabular)\n",
        "\n",
        "# Modelo para características tabulares\n",
        "tabular_model = Model(inputs=input_tabular, outputs=tabular_features)\n",
        "\n",
        "# Capas densas para características de imágenes\n",
        "image_features = base_model.output\n",
        "image_features = Flatten()(image_features)\n",
        "image_features = Dense(1000, activation='relu')(image_features)\n",
        "# image_features = Dropout(0.23204909844501467)(image_features)\n",
        "\n",
        "# Modelo para características de imágenes\n",
        "image_model = Model(inputs=base_model.input, outputs=image_features)\n",
        "\n",
        "# Concatenar las salidas de los modelos\n",
        "merged_features = Concatenate()([image_model.output, tabular_model.output])\n",
        "\n",
        "# Capa de regresión lineal\n",
        "output_layer = Dense(1, activation='linear', name='regression')(merged_features)\n",
        "\n",
        "# Modelo final de late fusion\n",
        "model = Model(inputs=[image_model.input, tabular_model.input], outputs=output_layer)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer=Adam(lr), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit([X_train_resized, X_train_tab], y_train_tab, validation_data=([X_val_resized, X_val_tab], y_val_tab), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluamos el modelo en el conjunto de prueba\n",
        "scores = model.evaluate([X_test_resized, X_test_tab], y_test_tab, verbose=1)\n",
        "print('Test MSE:', scores[0])\n",
        "print('Test MAE:', scores[1])\n",
        "\n",
        "# Predicciones en train\n",
        "y_train_pred = model.predict([X_train_resized, X_train_tab])\n",
        "r2_train = r2_score(y_train_tab, y_train_pred)\n",
        "print(f'R^2 Train: {r2_train}')\n",
        "\n",
        "# Predicciones en test\n",
        "y_test_pred = model.predict([X_test_resized, X_test_tab])\n",
        "r2_test = r2_score(y_test_tab, y_test_pred)\n",
        "print(f'R^2 Test: {r2_test}')\n",
        "\n",
        "# Deshacer el escalado de etiquetas\n",
        "\"\"\" y_train_tab = scaler.inverse_transform(y_train_tab)\n",
        "    y_val_tab = scaler.inverse_transform(y_val_tab)\n",
        "    y_test_tab = scaler.inverse_transform(y_test_tab) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conclusión Final\n",
        "\n",
        "\"\"\" En líneas generales creo que he estado condicionado por las limitaciones de la RAM a la hora de poder incluir más imágenes\n",
        "    o aumentar su resolución para conseguir mejores resultados. Lo mismo con la búsqueda de hiperparámetros, pero entiendo que esto\n",
        "    también será un problema del día a día en el trabajo de las empresas, asíque habrá que buscar ese equilibrio entre\n",
        "    el tamaño de los datos y la capacidad del procesamiento para conseguir los mejores resultados \"\"\""
      ],
      "metadata": {
        "id": "ZA9q5vEchIpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script de subida de imágenes y precios a Google Drive\n",
        "\n",
        "\"\"\" import cv2\n",
        "import os\n",
        "from urllib import request, error\n",
        "\n",
        "# Directorio de destino en Google Drive para las imágenes\n",
        "image_destination_dir = '/content/drive/MyDrive/imagenes_madrid/'\n",
        "\n",
        "# Directorio de destino en Google Drive para los datos de precios\n",
        "price_destination_dir = '/content/drive/MyDrive/precios_madrid/'\n",
        "\n",
        "# Crea los directorios de destino si no existen\n",
        "if not os.path.exists(image_destination_dir):\n",
        "    os.makedirs(image_destination_dir)\n",
        "\n",
        "if not os.path.exists(price_destination_dir):\n",
        "    os.makedirs(price_destination_dir)\n",
        "\n",
        "# Itera sobre las filas del DataFrame\n",
        "for index, row in airbnb_data.iterrows():\n",
        "    # Obtiene la URL de la columna \"Picture Url\"\n",
        "    url = row['Thumbnail Url']\n",
        "\n",
        "    # Genera el nombre de archivo basado en el índice\n",
        "    image_filename = f'image_{index}.jpg'\n",
        "\n",
        "    # Descarga la imagen y guárdala en Google Drive\n",
        "    try:\n",
        "        request.urlretrieve(url, os.path.join(image_destination_dir, image_filename))\n",
        "        print(\"Imagen descargada:\", image_filename)\n",
        "\n",
        "        # Obtiene el precio de la columna \"Price\"\n",
        "        price = row['Price']\n",
        "\n",
        "        # Genera el nombre de archivo para el precio basado en el índice\n",
        "        price_filename = f'price_{index}.txt'\n",
        "\n",
        "        # Guarda el precio en un archivo de texto en Google Drive\n",
        "        try:\n",
        "            with open(os.path.join(price_destination_dir, price_filename), 'w') as f:\n",
        "                f.write(str(price))\n",
        "            print(\"Precio guardado:\", price_filename)\n",
        "        except Exception as e:\n",
        "            print(\"Error al guardar el precio\", price_filename, \":\", str(e))\n",
        "\n",
        "    except error.URLError as e:\n",
        "        print(\"Error al descargar la imagen\", image_filename, \":\", str(e))\n",
        "    except Exception as e:\n",
        "        print(\"Error inesperado al descargar la imagen\", image_filename, \":\", str(e))\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "_x_u0jfxfn5T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}